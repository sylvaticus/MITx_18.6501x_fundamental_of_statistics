##  Lecture 23: Principal Component Analysis

### 23.01. Objectives

At the end of this lecture, you will be able to do the following:

- Understand the need for dimensionality reduction .
- Know how the empirical covariance matrix is used as a tool for dimensionality reduction.
- Understand spectral decomposition (without proof) of positive semi-definite matrices.
- Understand the role played by eigenvalues and eigenvectors in principal component analysis (PCA) .
- Use the PCA algorithm for dimensionality reduction.
- Use heuristics to determine the number of dimensions one must retain after performing PCA.

### 22.0
### 22.02. Introduction
### 22.03. Multivariate Statistics and Geometry Behind the Empirical Covariance
### 22.04. Geometry Behind the Empirical Covariance Matrix - Continued
### 22.05. Review of Linear Algebra Required for this Lecture
### 22.06. Principal Component Analysis (PCA) - Theorem
### 22.07. Largest Eigenvalue and Principal Directions
### 22.08. The PCA Algorithm
### 22.09. Beyond PCA
